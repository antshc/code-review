# Bottlenecks

## ğŸ’¡ Core Theses â€“ Deep Insights and Real-World Anchors

### **1. â€œBad code reviews are symptoms, not root causes.â€**

**Meaning:**  
Toxic or broken reviews stem from unclear processes, lack of shared expectations, and weak psychological safety.

**Mechanism:**
- **Lazy reviews** â†’ â€œLGTMâ€ comments on unread code.
- **Mean reviews** â†’ personal attacks instead of feedback.
- **Shape-shifting reviews** â†’ code keeps changing mid-review.
- **Stringent reviews** â†’ excessive bureaucracy without automation.

**Examples:**
1. A senior dev skims 40 PRs a week â†’ critical bug slips through.  
2. Reviewer says â€œDo you even know async?â€ â†’ new dev loses confidence.  
3. PR keeps changing every hour â†’ reviewer fatigue and resentment.

**Lesson:** Fix the **system**, not the **symptom** â€” automate checks, set etiquette, and define review readiness.

---

### **2. â€œBottlenecks kill trust and throughput.â€**

**Meaning:**  
The â€œSingle Senior Developer Reviewerâ€ problem creates burnout, delays, and dependency.

**Mechanism:**
- Overreliance on one expert delays reviews.  
- Juniors never learn; team velocity drops.  

**Solutions:**
- Rotate reviewers; empower all devs to approve small PRs.  
- Safeguards: automated tests, CI/CD gates, and rollback mechanisms.  
- Encourage juniors to review as a learning exercise.  

**Real-world parallel:**  
> â€œIf only one person can merge code, you donâ€™t have a process â€” you have a single point of failure.â€

---

### **3. â€œPreparation equals respect.â€**

**Meaning:**  
Good reviews start with good pull requests. Unclear PRs waste everyoneâ€™s time.

**Mechanism:**
- Missing descriptions or unrelated changes confuse reviewers.  
- Oversized diffs delay feedback.

**Examples:**
- A 3,000-line PR combines refactoring + feature = impossible to review.  
- A PR without explanation = guesswork and frustration.

**Practices:**
- Limit PRs to â‰¤500 LOC or â‰¤30 minutes review time.  
- Use PR templates and validation bots.  
- Write meaningful titles: â€œFix: Cache invalidation in BlobReaderâ€ > â€œMinor fix.â€

---

### **4. â€œDiscussion is valuableâ€”but endless debate is waste.â€**

**Meaning:**  
Asynchronous comment wars kill productivity and morale.

**Mechanism:**
- Text-only debates lack tone and empathy.  
- Async comments spiral into ego battles.

**Examples:**
- 47 comments debating variable naming.  
- Two engineers arguing over tab vs space for three days.

**Solutions:**
- Move to a 5-minute call after 2â€“3 comment rounds.  
- Summarize outcomes in PR comments for transparency.  
- Use â€œConventional Commentsâ€ and MoSCoW categories to structure dialogue.

> ğŸ’¬ *â€œMove the heat from text to talk â€” then document the peace.â€*

---

### **5. â€œBig features must be split before they reach review.â€**

**Meaning:**  
Oversized features reveal planning or design gaps â€” not just bad review discipline.

**Mechanism:**
- Poor decomposition leads to mega-PRs.  
- Reviewers canâ€™t give meaningful feedback at scale.

**Examples:**
- â€œImplement billing systemâ€ PR touches 70 files.  
- Acceptance criteria missing â†’ unclear scope.

**Best Practices:**
- Split by domain (UI, backend, API).  
- Use **feature flags** for incremental rollout.  
- Apply Gitâ€™s `cherry-pick` or `rebase` to create atomic PRs.  
- Discuss breakdown strategies during design, not review.

---

### **6. â€œA living process prevents loopholes.â€**

**Meaning:**  
Even good review processes decay over time without maintenance.

**Mechanism:**
- Undefined workflows, outdated tools, or approval vanity metrics.  
- â€œEmergency mergesâ€ become normal.

**Examples:**
- Skipped reviews during hotfixes never documented.  
- Teams measure â€œPRs merged per weekâ€ instead of review quality.

**Solutions:**
- Maintain a **Team Working Agreement (TWA)**.  
- Audit automation and approval rules quarterly.  
- Introduce an **Emergency Playbook** for justified review bypasses.  
- Conduct post-mortems to ensure accountability.

**Leadership Connection:**  
Adaptive governance = clear rules + psychological trust. High-performing teams revisit both regularly.

---

## ğŸ§  Main Takeaways

1. **Code reviews are cultural mirrors** â€” not just technical gates.  
2. **Structure enables empathy** â€” automation frees humans for judgment.  
3. **Psychological safety drives feedback quality.**  
4. **Review processes need periodic tuning.**  
5. **Measure outcomes, not output** â€” value insights over velocity.

---

## ğŸ§© Real-World Scenarios

| Dilemma | Example | Remedy |
|----------|----------|--------|
| Lazy reviews | â€œLGTMâ€ on unread PRs | Require review notes or checklist acknowledgment |
| Mean reviews | â€œWho wrote this mess?â€ | Enforce constructive tone and objective phrasing |
| Senior bottleneck | One reviewer for all PRs | Rotate reviewers; mentor juniors |
| Large PRs | 2,000+ LOC | Split by feature flags and submodules |
| Endless debate | Dozens of comments | Jump to call â†’ summarize resolution |
| Process loopholes | Emergency merges | Use documented Emergency Playbook |
| Cultural burnout | â€œWe hate PRsâ€ attitude | Automate routine checks; celebrate great reviews |

---

## ğŸ¯ Strategic and Leadership Implications

- **Leadership Principle:** A teamâ€™s communication quality defines its review quality.  
- **Strategic Insight:** Review processes are *organizational architecture* â€” they shape how information and trust flow.  
- **Decision-Making Link:** Dilemmas = trade-offs between speed, quality, and trust. High-performing teams make these trade-offs visible and revisited.

---

## ğŸ§± Example-Driven Practices

1. **Convert conflict to collaboration:**  
   â€œYou forgot testsâ€ â†’ â€œLetâ€™s add a test for X â€” hereâ€™s a pattern.â€  

2. **Empower juniors early:**  
   Let new hires approve docs or tests to build confidence.  

3. **Turn reviews into learning rituals:**  
   Run a 30-min â€œPR Walkthrough Fridayâ€ for large PRs.  

4. **Use Emergency Playbooks:**  
   Skip review for critical Friday hotfix, then document post-mortem Monday.

---

## ğŸ’¬ Integration with Complementary Insights

- **Trisha Gee:** Human reviewers should focus on *design, readability, maintainability*, not style or formatting â€” automate that.  
- **Tess Ferrandez-Norlander:** â€œFeedback is communication.â€ Be open to feedback, write reviewable PRs, settle style wars in a guide, and make small, self-contained PRs.  
- **Adrienne Tacke:** Code reviews are not just technical correctness checks â€” they are *cultural glue.*

---

## ğŸ§­ Final Reflection

> **â€œGood teams review code. Great teams review how they review code.â€**

Part 3 reminds us that the goal is not to make code perfect â€” itâ€™s to make teams stronger. When feedback is respectful, processes are adaptive, and automation removes friction, code reviews evolve from a pain point into a *collective learning engine.*

---
