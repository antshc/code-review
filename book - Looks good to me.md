## Communication Qualities in Code Reviews

### Team Working Agreement (TWA) â€” Short Explanation & Example

#### ğŸ”¹ What is a TWA?
A **Team Working Agreement (TWA)** is a shared set of rules and expectations the team creates together to define **how we work**.  
It aligns communication, collaboration, and quality standards.

**Easy recall:**  
ğŸ‘‰ *TWA = â€œHow we work together.â€*

#### ğŸ”¹ What a TWA Is Not
- Not a process document  
- Not management rules  
- Not imposed by one person  

It is **co-created** and evolves with the team.

#### ğŸ”¹ What a TWA Usually Covers
- Communication expectations  
- PR & code review rules  
- Meeting etiquette  
- Quality and Definition of Done  
- Collaboration norms  
- Conflict resolution rules  

---

#### ğŸ”¹ Example â€” E-commerce Checkout Squad TWA
```
##### **1. Communication**
- Respond to team messages within 2 working hours.  
- Use PR comment prefixes: `question:`, `suggestion:`, `nit:`, `praise:`.

##### **2. Pull Requests**
- PRs must include test results and reasoning.  
- No withholding approvals without clear feedback. (Withholding approval means **delaying or blocking** a PR **without providing clear, actionable feedback**.  
This slows delivery and creates friction.)

##### **3. Meetings**
- Start on time; avoid multitasking.  
- Document decisions immediately.

##### **4. Work Style**
- Pair on complex tasks (>1 day).  
- Ask for help after 30 minutes of being stuck.

##### **5. Quality**
- Code must follow standards and include unit tests.  
- A feature is â€œDoneâ€ only after testing in staging.
```


### 1. Use constructive tone and phrasing (â€œweâ€ instead of â€œyouâ€)

**Meaning:**
Replace personal or accusatory phrasing (â€œYou forgot to handle nullsâ€) with collaborative, team-oriented phrasing (â€œCould we add a null check here?â€). It signals shared ownership, reduces defensiveness, and promotes an inclusive learning culture. The tone shapes the teamâ€™s emotional safety and directly affects willingness to engage in reviews.

**Connection to real practice:**
Constructive tone supports long-term team trust. In distributed or asynchronous teams, written tone replaces body language â€” so phrasing *is* body language. Teams that consistently use inclusive language see higher participation in reviews and lower turnover of junior devs.

**Examples:**

1. âŒ â€œYou didnâ€™t follow the naming standard.â€ â†’ âœ… â€œCan we align this name with our convention from the TWA?â€
2. âŒ â€œThis function is confusing.â€ â†’ âœ… â€œWe might make this function easier to follow by splitting the logic.â€
3. âŒ â€œYou should fix this test.â€ â†’ âœ… â€œCould we pair on updating the test to clarify the intent?â€

**Lesson learned:** Words drive psychological safety. Review tone determines whether developers perceive feedback as *a challenge* or *a collaboration*.

---

### 2. Apply structured feedback formats (Conventional Comments, MoSCoW)

**Meaning:**
Structure brings clarity and prioritization. Conventional Comments standardize tone (â€œnit:â€, â€œsuggestion:â€, â€œquestion:â€), while MoSCoW (â€œMustâ€, â€œShouldâ€, â€œCouldâ€, â€œWonâ€™tâ€) communicates importance levels. Both prevent emotional misinterpretation and make responses easier to prioritize.

**Why comments are important:**
Code review comments are the main vehicle of knowledge transfer, mentorship, and quality alignment. They shape team culture, set expectations for future work, and record design decisions. Each comment contributes to the shared language and history of the codebase.

**What makes a comment effective:**

* Focused on code, not the coder.
* Clear about *what* and *why*, not just *how*.
* Respectful in tone and phrased collaboratively.
* Actionable, verifiable, and traceable.

**How to write effective comments:**

* Start with context (why it matters).
* State the suggestion succinctly.
* Offer reasoning or alternative examples.
* Use structured labels for clarity: `question:`, `suggestion:`, `nit:`.
* Apply MoSCoW categories for urgency and impact.

**5P Process for evaluating suggestions:**

<img width="320" height="280" alt="image" src="https://github.com/user-attachments/assets/0543dd72-89f5-40a9-9451-87c074b2e31d" />

1. **Purpose** â€“ Does it align with the team or business goal?
2. **Principle** â€“ Is it consistent with coding standards or architecture?
3. **Practicality** â€“ Is it feasible to implement in scope or time?
4. **Performance** â€“ Will it improve quality or efficiency?
5. **People** â€“ Will it help the team learn or collaborate better?
   If at least three of these Ps are positive, the suggestion is valid to add.

**MMG Exchange (Meaningâ€“Mechanismâ€“Goal):**

Explanation: https://github.com/antshc/code-review/blob/main/MMG-Exchange-solve-conflicts.adoc

A framework for resolving conflicting â€œobjectiveâ€ views:

* *Meaning:* clarify what each developer interprets the issue to mean.
* *Mechanism:* explain how each solution works technically.
* *Goal:* restate the shared end goal.
  This ensures both sides can see logic rather than emotion, leading to mutual clarity.

**Comment signals & categories:**

* Use **MoSCoW** to prioritize: Must / Should / Could / Wonâ€™t.
* Apply **Conventional Comments** prefixes: `question:`, `nit:`, `suggestion:`, `praise:`. https://github.com/antshc/code-review/blob/main/conventional-comments.adoc
* Combine them to form clear, ordered communication, e.g., `Must â€“ suggestion:` or `Could â€“ question:`.

**The Tripleâ€‘R pattern for requesting changes:**

Explanation: https://github.com/antshc/code-review/blob/main/Triple-R%20Pattern%20feedback.adoc

1. **Recognize** â€“ Acknowledge what works well.
2. **Reason** â€“ Explain why a change helps (context, risk, benefit).
3. **Request** â€“ Make a specific, respectful ask for change.

**Examples:**

1. *Conventional Comment:* â€œ**suggestion:** Could we extract this block to a helper for readability?â€
2. *MoSCoW:* â€œ**Must:** Add validation for negative values before merge. **Could:** Replace `List` with `HashSet` for faster lookup later.â€
3. *Tripleâ€‘R example:* â€œNice modular design! Since this loop is O(nÂ²), consider caching results for performance.â€

**Lesson learned:** Effective comments create clarity, empathy, and traceable action. Standardized phrasing and structured reasoning build predictability and help reviewers spend less mental energy deciphering tone and intent â€” increasing review throughput.

---

### 3. Include positive reinforcement for well-written code

**Meaning:**
Balance critique with appreciation. Acknowledge good structure, clear naming, and well-documented logic. Reinforcement encourages repetition of good patterns and fosters motivation, especially for junior or new contributors.

**Connection to real-world practice:**
Positive reinforcement turns code reviews into *learning moments*, not *judgments*. Teams that highlight good examples in PRs create â€œlive style guidesâ€ â€” practical references of what â€œgoodâ€ looks like in the project.

**Examples:**

1. â€œGreat test coverage here â€” the edge case for null inputs is a good addition.â€
2. â€œLove this use of `using` â€” this pattern reduces risk of leaks.â€
3. â€œNice refactor; smaller method makes intent clearer.â€

**Lesson learned:** Praise is a multiplier. Reinforcing good practices makes reviewers *teachers* and authors *motivated learners*.

---

### 4. Avoid review anti-patterns: lazy, mean, shape-shifting, and stringent reviews

**Meaning:**
Bad review behaviors destroy trust and slow velocity:

* **Lazy:** â€œLGTMâ€ with no context or review effort.
* **Mean:** Harsh or personal comments (â€œDid you even test this?â€).
* **Shape-shifting:** Constantly changing PR scope, causing re-reviews.
* **Stringent:** Bureaucratic review processes with manual gates or micromanagement.

**Connection to practice:**
These patterns cause resentment and process fatigue. Preventing them is a leadership issue â€” requiring social norms (Team Working Agreement) and automation to remove repetitive checks.

**Examples:**

1. *Lazy:* Reviewer writes only â€œLGTMâ€. â†’ **Fix:** Require brief rationale (â€œLGTM â€“ ran tests locally and verified new config worksâ€).
2. *Mean:* â€œThis code is terrible.â€ â†’ **Fix:** â€œThe function is complex; could we extract smaller methods to improve readability?â€
3. *Shape-shifting:* PR constantly updated during review. â†’ **Fix:** Use draft PRs until ready and freeze scope once review starts.

**Lesson learned:** Consistency and empathy are productivity tools. Removing negative patterns saves time and morale.

---

### 5. Move to synchronous discussion after excessive back-and-forth

**Meaning:**
When PR discussions become long threads or circular arguments, shift to real-time conversation (chat, call, pair session). Written async feedback loses nuance and breeds misunderstanding.

**Connection to decision-making:**
Timely voice conversations speed resolution of design disagreements and preserve delivery velocity. Recording or summarizing outcomes in the PR keeps transparency and institutional memory.

**Examples:**

1. After 10+ comments about design choice, comment:
   â€œLetâ€™s sync for 10 min to clarify context â€” Iâ€™ll summarize results in PR afterward.â€
2. Two reviewers disagree â†’ quick 3-way call resolves in minutes.
3. Large feature with multiple affected modules â†’ schedule mini â€œmob reviewâ€ session to walk through logic together.

**Lesson learned:** Async comments scale knowledge; sync talks resolve tension. The best teams balance both.

---

### 6. Summarize offline resolutions in PR comments for transparency

**Meaning:**
After an offline or voice discussion, add a summary comment of what was decided, why, and any next steps. This closes the loop and avoids â€œhiddenâ€ decisions that others canâ€™t see.

**Connection to real-world collaboration:**
Summarizing outcomes improves traceability, reduces repeated questions, and teaches others. It also prevents â€œsiloedâ€ tribal knowledge â€” critical in regulated or multi-team environments.

**Examples:**

1. â€œDiscussed with @Maria offline â€” weâ€™ll keep this config local until next release; adding TODO in backlog.â€
2. â€œSynced with reviewer: validation logic stays in service layer per design doc.â€
3. â€œOutcome: no change required; doc updated to clarify naming convention.â€

**Lesson learned:** Written closure is professional discipline. It transforms transient talk into durable team memory.

---
### 7. PR Templates
PR templates are preset outlines of questions or checklist items that are automatically generated when a PR is created. This is an effective tactic to consistently produce properly prepared PRs. 
Template example: https://github.com/antshc/code-review/blob/main/pr-description-with-reasoning.md

```text
- [ ] Description â€“ Why is this change being made?  
      (Please add any relevant context, history, or decisions related to this feature)
- [ ] Linked Work Ticket/Issue #
- [ ] Linked Documentation
- [ ] Accompanying unit/integration tests included
- [ ] Testing evidence included
- [ ] Minimum of 2 reviewers assigned
```
---
## Main Takeaways

| Principle              | Impact on Culture                          | Measurable Effect                |
| ---------------------- | ------------------------------------------ | -------------------------------- |
| Constructive phrasing  | Builds psychological safety                | Higher PR participation rate     |
| Structured feedback    | Makes review actionable and auditable      | Shorter review cycles            |
| Positive reinforcement | Increases motivation and learning          | Better code quality consistency  |
| Avoiding anti-patterns | Prevents review fatigue and resentment     | Reduced PR rejection rate        |
| Synchronous resolution | Speeds alignment and removes tone friction | Fewer re-openings or escalations |
| PR outcome summaries   | Ensures transparency and shared knowledge  | Fewer duplicate discussions      |

**Overall lesson:**

> *Effective communication is the backbone of effective code reviews.*
> It turns reviews from gatekeeping into mentorship, from friction into collaboration, and from opinion into shared decision-making.
